{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnEP6rQgDCn6"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# https://huggingface.co/blog/annotated-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF9om8myFRKz"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuYo4Kw2DCn8"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# use seed for reproducability\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Convert to tensor and normalize between -1 and 1\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "batch_size = 1\n",
        "channels = 1\n",
        "height = 28\n",
        "width = 28\n",
        "\n",
        "# Load mnist without labels\n",
        "\n",
        "mnist_train = MNIST('./data', train=True, download=True, transform=transform)\n",
        "mnist_test = MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create a data loader with only 2 training samples\n",
        "subset = 1\n",
        "\n",
        "mnist_subset = torch.utils.data.Subset(mnist_train, range(subset))\n",
        "overfit_loader = DataLoader(mnist_subset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split mnist_test in half and create test and validation data loaders\n",
        "mnist_test, mnist_val = torch.utils.data.random_split(mnist_test, [5000, 5000])\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Val set used to validate model\n",
        "val_loader = DataLoader(mnist_val, batch_size=len(mnist_val), shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "overfit = True\n",
        "if overfit:\n",
        "    train_loader = overfit_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Ktv-wXCdDCn9",
        "outputId": "3131db1f-482f-4ec0-83cc-10f10e4e7ff5"
      },
      "outputs": [],
      "source": [
        "image = next(iter(train_loader))[0][0]\n",
        "plt.imshow(image.squeeze(), cmap='gray');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwrR1HGFDCn-"
      },
      "outputs": [],
      "source": [
        "T = 200\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlj24mcjA97q"
      },
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "4rFLV8zxDCn-",
        "outputId": "570122fe-3b35-4e33-d013-10957c98b1f3"
      },
      "outputs": [],
      "source": [
        "betas_lin = linear_beta_schedule(T)\n",
        "betas_cos = cosine_beta_schedule(T)\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
        "ax[0].plot(betas_lin)\n",
        "ax[1].plot(betas_cos)\n",
        "ax[2].plot(1-betas_cos.flip(0))\n",
        "\n",
        "# betas = betas_cos * 0.3\n",
        "betas = betas_lin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k--JawtVDCn_"
      },
      "outputs": [],
      "source": [
        "# Constant tensors used in sampling functions\n",
        "\n",
        "def get_constants(betas):\n",
        "  alphas = 1.0 - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "  alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1,0), value=1.0)\n",
        "  sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "  sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "  sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "  posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "  return  {'alphas_cumprod': alphas_cumprod, \n",
        "          'sqrt_recip_alphas': sqrt_recip_alphas, \n",
        "          'sqrt_alphas_cumprod': sqrt_alphas_cumprod, \n",
        "          'sqrt_one_minus_alphas_cumprod': sqrt_one_minus_alphas_cumprod,\n",
        "          'posterior_variance': posterior_variance}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "YeqM1kaPDCoA",
        "outputId": "80cfbc8f-4e5d-4590-d2a6-02e9f4a40259"
      },
      "outputs": [],
      "source": [
        "# Extract appropriate t index from batch of alphas (?)\n",
        "def extract(a: torch.tensor, t: torch.tensor, x_shape: torch.tensor):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "def q_sample(x_start, t, betas, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    consts = get_constants(betas)\n",
        "    sqrt_alphas_cumprod = consts['sqrt_alphas_cumprod']\n",
        "    sqrt_one_minus_alphas_cumprod = consts['sqrt_one_minus_alphas_cumprod']\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "    \n",
        "    return x_start * sqrt_alphas_cumprod_t + noise * sqrt_one_minus_alphas_cumprod_t\n",
        "\n",
        "idx = torch.randint(0, T, (1,))\n",
        "img_noisy = q_sample(image, torch.tensor([idx]), betas)\n",
        "print(img_noisy.shape)\n",
        "print(f\"Noise step: {idx.item()}\")\n",
        "plt.imshow(img_noisy.squeeze(), cmap='gray');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7tSu19HDCoB"
      },
      "outputs": [],
      "source": [
        "# Define some useful helper stuff\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x + self.fn(x, *args, **kwargs)\n",
        "\n",
        "def default(value, default):\n",
        "    if value is not None:\n",
        "        return value\n",
        "    \n",
        "    return default() if isfunction(default) else default\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, kernel_size=4, stride=2, padding=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFX1N128DCoB"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Class for time embedding a batch of time steps\n",
        "    into a tensor of shape (batch_size, dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time: torch.tensor, max_period: int = 10_000):\n",
        "        # time: (batch_size,)\n",
        "        # embeddings: (batch_size, dim)\n",
        "\n",
        "        # assert time.dim() == 1\n",
        "\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = time[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        \n",
        "        return emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "Ep39Z95KDCoD",
        "outputId": "7de3c517-30c4-43a8-eaf3-129c0d67d3cd"
      },
      "outputs": [],
      "source": [
        "t = torch.arange(0, 200)\n",
        "print(t.shape)\n",
        "sinusoidal_embedding = SinusoidalPositionEmbeddings(128)\n",
        "emb = sinusoidal_embedding(t)\n",
        "print(emb.shape)\n",
        "\n",
        "# plot the embeddings\n",
        "plt.imshow(emb.detach().numpy());"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnRbQD0iDCoD"
      },
      "outputs": [],
      "source": [
        "# Blocks for U-net\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups=8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, kernel_size=3, padding=1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift: tuple = None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if scale_shift is not None:\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        return self.act(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "            if time_emb_dim is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb = None):\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if self.mlp is not None and time_emb is not None:\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "\n",
        "class ConvNextBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
        "            if time_emb_dim is not None else None\n",
        "        )\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.GroupNorm(1, dim_out * mult),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        # print(x.shape)\n",
        "        h = self.ds_conv(x)\n",
        "        # print(h.shape)\n",
        "\n",
        "        if self.mlp is not None and time_emb is not None:\n",
        "            # print(\"Time emb shape in ConvNext:\", time_emb.shape)\n",
        "            condition = self.mlp(time_emb)\n",
        "            # print(condition.shape)\n",
        "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9LSD7q3DCoE"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.head = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1) # Learn Q, K, V jointly\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.head), qkv\n",
        "            )\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "            )\n",
        "    \n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PplJ7nvPDCoE"
      },
      "outputs": [],
      "source": [
        "# Layer for computing groupnorm before a function fn\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kHp3OPJDCoF"
      },
      "outputs": [],
      "source": [
        "# Define U-net\n",
        "# takes in a batch of images: (batch_size, channels, height, width)\n",
        "# and returns a batch of images: (batch_size, channels, height, width)\n",
        "\n",
        "# Conv on input + positional embeddings\n",
        "# 2 x ConvNextBlock + GroupNorm + attention + residual + downsample x N\n",
        "# middle: ConvNeXT + attention \n",
        "# decoding: 2 x ConvNextBlock + GroupNorm + attention + residual + upsample x N\n",
        "# final: ConvNeXT + conv\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim=None,\n",
        "        out_dim=None,\n",
        "        dim_mults=(1,2,4,8),\n",
        "        channels=1,\n",
        "        resnet_block_groups=8,\n",
        "        convnext_mult=2,\n",
        "        use_convnext=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        init_dim = init_dim if init_dim else dim // 3 * 2\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "        # create list of dimensions for each block\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        if use_convnext:\n",
        "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
        "        else:\n",
        "            block_klass = partial(ResBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # print(\"Using block class:\", block_klass)\n",
        "        # Time embedding\n",
        "        time_dim = dim * 4\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(dim),\n",
        "            nn.Linear(dim, time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "        )\n",
        "\n",
        "        # create list of blocks\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out) # 'depth'\n",
        "\n",
        "        # Downsample\n",
        "        for i, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = i >= num_resolutions - 1\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList([\n",
        "                    block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                    block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                    Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                    Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "                ]))\n",
        "        \n",
        "        # Middle part\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        # Upsample\n",
        "        for i, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = i >= num_resolutions - 1\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList([\n",
        "                    block_klass(dim_out*2, dim_in, time_emb_dim=time_dim),\n",
        "                    block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                    Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                    Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "                ]))\n",
        "        \n",
        "        # Output\n",
        "        out_dim = out_dim if out_dim else channels\n",
        "        self.final_conv = nn.Sequential(\n",
        "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1))\n",
        "\n",
        "    def forward(self, x, time):\n",
        "\n",
        "        # print(\"Input shape:\", x.shape)\n",
        "        # print(\"Time shape:\", time.shape)\n",
        "        x = self.init_conv(x)\n",
        "        t = self.time_mlp(time)\n",
        "        h = [] # skip connections?\n",
        "\n",
        "        # Downsample\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        # Middle\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        # Upsample\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "        \n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unet = Unet(\n",
        "    dim=28,\n",
        "    channels=channels,\n",
        "    dim_mults=(1,2,4,),\n",
        "    resnet_block_groups=7,\n",
        "    use_convnext=False,\n",
        "    )\n",
        "\n",
        "unet.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = torch.randint(0, T, (batch_size,)).to(device)\n",
        "\n",
        "x1 = next(iter(train_loader))[0].to(device)\n",
        "print(x1.shape)\n",
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "R9xkmMBZDCoG",
        "outputId": "3423177b-ca52-4a2a-809d-4d1db3805fbb"
      },
      "outputs": [],
      "source": [
        "# call unet on image and time tensor\n",
        "\n",
        "# out = unet(x1, t)\n",
        "out = unet(x1, torch.tensor([0.0]).to(device))\n",
        "\n",
        "# plot x1 and out side by side\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(x1[0, 0, :, :].cpu().detach().numpy());\n",
        "ax[1].imshow(out[0, 0, :, :].cpu().detach().numpy());"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLBQsrXvDCoG",
        "outputId": "9b75e5fd-f121-4db7-b25b-1aeb747f8afe"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "xsize = x1[0].size()\n",
        "tsize = t[0].size()\n",
        "# call summary on unet\n",
        "#summary(unet, [(128, 1, 28, 28), (128,)], depth=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN_64SGLDCoG"
      },
      "outputs": [],
      "source": [
        "from plot_utils import row_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "9QlexvxEDCoH",
        "outputId": "25b34f3c-c0ba-4ea2-fae8-fbb55fba7a2d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "image = next(iter(train_loader))[0][0]\n",
        "print(image.shape)\n",
        "n_steps = 20\n",
        "noised_ims_lin = [q_sample(image, torch.tensor([t]), betas=betas_lin) for t in np.linspace(0, T-1, n_steps, dtype=int)]\n",
        "print(noised_ims_lin[0].shape)\n",
        "row_plot(noised_ims_lin)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "qCbcoy1hCxsO",
        "outputId": "e19b32c6-3f12-4f43-f4ec-c26790443804"
      },
      "outputs": [],
      "source": [
        "noised_ims_cos = [q_sample(image, torch.tensor([t]), betas=betas_cos) for t in np.linspace(0, T-1, n_steps, dtype=int)\n",
        "]\n",
        "print(noised_ims_cos[0].shape)\n",
        "row_plot(noised_ims_cos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "noised_ims_cos = [q_sample(image, torch.tensor([t]), betas=betas_cos*0.3) for t in np.linspace(0, T-1, n_steps, dtype=int)\n",
        "]\n",
        "print(noised_ims_cos[0].shape)\n",
        "row_plot(noised_ims_cos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot outputs of model for 10 evenly spaced time steps\n",
        "n_steps = 10\n",
        "test_outputs = [unet(image, torch.tensor([t])).detach()[0, :, :, :] for t in np.linspace(0, T-1, n_steps, dtype=int)]\n",
        "row_plot(test_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YurG6R5UDCoH"
      },
      "outputs": [],
      "source": [
        "def p_losses(model, x_start, t, betas, noise=None, loss_type=\"l1\"):\n",
        "    \"\"\" Computes the loss for a given model and batch of imgs.\n",
        "    Args:   x_start: the input image (batch_size, channels, height, width)\n",
        "            t: the time (batch_size,)\n",
        "            noise: the noise to add to the input image (batch_size, channels, height, width)\n",
        "    \"\"\"\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise, betas=betas)\n",
        "    predicted_noise = model(x_noisy, t)\n",
        "\n",
        "    if loss_type == \"l1\":\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"l2\":\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise ValueError(\"Loss type not supported.\")\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_noise = torch.randn_like(image)\n",
        "test_noise2 = torch.randn_like(image)\n",
        "F.smooth_l1_loss(test_noise, test_noise2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE6MrjqoDCoI",
        "outputId": "2fd1dbde-05d4-427b-f6fc-13a8ff05cca2"
      },
      "outputs": [],
      "source": [
        "im = next(iter(train_loader))[0].to(device)\n",
        "print(im.shape)\n",
        "\n",
        "out = p_losses(unet, im, t, betas, loss_type=\"huber\")\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jUFpQoqDCoI"
      },
      "outputs": [],
      "source": [
        "# Sampling during training\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index, betas):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "\n",
        "    consts = get_constants(betas)\n",
        "    sqrt_one_minus_alphas_cumprod = consts['sqrt_one_minus_alphas_cumprod']\n",
        "    sqrt_recip_alphas = consts['sqrt_recip_alphas']\n",
        "    posterior_variance = consts['posterior_variance']\n",
        "\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "    # Predict mean\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    # If we're on last sample\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "\n",
        "        return model_mean + noise * torch.sqrt(posterior_variance_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLLr1_W9DCoI",
        "outputId": "463746dc-25d8-4be0-ea53-868a61558f26"
      },
      "outputs": [],
      "source": [
        "tstep = torch.tensor([100]).to(device)\n",
        "x100 = q_sample(im, tstep, betas)\n",
        "print(x100.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Aa8PGifDCoJ"
      },
      "outputs": [],
      "source": [
        "# Full sampling loop from noise to image\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape, betas):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    b = shape[0]\n",
        "\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "\n",
        "    for i in tqdm(reversed(range(0, T)), desc=\"Sampling\"):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i, betas)\n",
        "        imgs.append(img)\n",
        "    return imgs\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, image_size, betas, batch_size=16, channels=1):\n",
        "    return p_sample_loop(model, (batch_size, channels, image_size, image_size), betas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLh1c_R_DCoJ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok=True)\n",
        "save_and_sample_every = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WtthXrsDCoK"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(unet.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQFuJkECDCoK",
        "outputId": "531a99ee-1340-4652-e886-adea7699ae4d"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "from tqdm import trange\n",
        "\n",
        "loader = train_loader\n",
        "epochs = 500\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "with trange(epochs) as pbar:\n",
        "    for epoch in pbar:\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Actually loading images and labels now in data_loader...\n",
        "\n",
        "            batch_size = batch[0].shape[0]\n",
        "            batch = batch[0].to(device)\n",
        "\n",
        "            # Sample t uniformally \n",
        "            t = torch.randint(0, T, (batch_size,), device=device).long()\n",
        "\n",
        "            loss = p_losses(unet, batch, t, betas, loss_type=\"huber\")\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                pbar.set_postfix(step=step, loss=loss.item())\n",
        "                losses.append(loss.item())\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Save generated images\n",
        "            #if step != 0 and step % save_and_sample_every == 0:\n",
        "            #    milestone = step // save_and_sample_every\n",
        "            #    batches = num_to_groups(4, batch_size)\n",
        "            #    all_images_list = list(map(lambda n: sample(unet, batch_size=n), batches))\n",
        "            #    all_images = torch.cat(all_images_list, dim=0)\n",
        "            #    save_image(all_images, str(results_folder / f\"sample_{milestone}.png\"), nrow=6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "c7EbXiOOvLDO",
        "outputId": "8ad1fd02-067c-4d1a-ec7f-74f0e19268bb"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "58a380122f3b45ffbc0af62ed654e2c4",
            "ba3ccfc5dc5e4a3b8b724e98db4bd487",
            "ca64c7e8b8f2491d8e2396c5485c66ca",
            "470a27879d93407b84960aa84ce2edbb",
            "ba662a00752147e5bb6bde9474d7f7c8",
            "c9a11613b892444f997f37dd2e2f9cfe",
            "c8639945a3f74de998a2603e7e34a62e",
            "6c73ba7d65a94e99b63c5286eb362078",
            "f4b676c075064314993417565db1f133",
            "737a14e9a87549f499c6699119374fb9",
            "4a375231ecd740fbbf8e1a09260f33a9"
          ]
        },
        "id": "O4MxCIguHX3R",
        "outputId": "8e5f242d-701b-41f7-9a12-eb30ec6ef03b"
      },
      "outputs": [],
      "source": [
        "# sample 16 images\n",
        "samples = sample(unet, betas=betas, image_size=28, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ims10 = samples[0:-1:20]\n",
        "row_plot([im.view(1, height, width) for im in ims10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "SuHiPb3pIplM",
        "outputId": "082018a9-345c-41fa-e008-bd843c9c6a44"
      },
      "outputs": [],
      "source": [
        "random_index = np.random.randint(0, 128)\n",
        "\n",
        "plt.imshow(samples[-1][0, 0, :, :].cpu());"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('thesis')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4379223da8825d70b8c2eb245c1476bc432c86a2a66eb506943eca18af8434f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "470a27879d93407b84960aa84ce2edbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_737a14e9a87549f499c6699119374fb9",
            "placeholder": "​",
            "style": "IPY_MODEL_4a375231ecd740fbbf8e1a09260f33a9",
            "value": " 200/? [00:07&lt;00:00, 28.68it/s]"
          }
        },
        "4a375231ecd740fbbf8e1a09260f33a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58a380122f3b45ffbc0af62ed654e2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba3ccfc5dc5e4a3b8b724e98db4bd487",
              "IPY_MODEL_ca64c7e8b8f2491d8e2396c5485c66ca",
              "IPY_MODEL_470a27879d93407b84960aa84ce2edbb"
            ],
            "layout": "IPY_MODEL_ba662a00752147e5bb6bde9474d7f7c8"
          }
        },
        "6c73ba7d65a94e99b63c5286eb362078": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "737a14e9a87549f499c6699119374fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba3ccfc5dc5e4a3b8b724e98db4bd487": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9a11613b892444f997f37dd2e2f9cfe",
            "placeholder": "​",
            "style": "IPY_MODEL_c8639945a3f74de998a2603e7e34a62e",
            "value": "Sampling: "
          }
        },
        "ba662a00752147e5bb6bde9474d7f7c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8639945a3f74de998a2603e7e34a62e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9a11613b892444f997f37dd2e2f9cfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca64c7e8b8f2491d8e2396c5485c66ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c73ba7d65a94e99b63c5286eb362078",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4b676c075064314993417565db1f133",
            "value": 1
          }
        },
        "f4b676c075064314993417565db1f133": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
