{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# https://huggingface.co/blog/annotated-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Convert to tensor and normalize between -1 and 1\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "# Load mnist\n",
    "mnist = MNIST('./data', train=True, download=True, transform=transform)\n",
    "data_loader = DataLoader(mnist, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = next(iter(data_loader))[0][0]\n",
    "plt.imshow(image.squeeze(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = linear_beta_schedule(T)\n",
    "plt.plot(betas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant tensors used in sampling functions\n",
    "\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1,0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract appropriate t index from batch of alphas (?)\n",
    "def extract(a: torch.tensor, t: torch.tensor, x_shape: torch.tensor):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "    \n",
    "    return x_start * sqrt_alphas_cumprod_t + noise * sqrt_one_minus_alphas_cumprod_t\n",
    "\n",
    "idx = torch.randint(0, T, (1,))\n",
    "img_noisy = q_sample(image, torch.tensor([idx]))\n",
    "print(img_noisy.shape)\n",
    "print(f\"Noise step: {idx.item()}\")\n",
    "plt.imshow(img_noisy.squeeze(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful helper stuff\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x + self.fn(x, *args, **kwargs)\n",
    "\n",
    "def default(value, default):\n",
    "    if value is not None:\n",
    "        return value\n",
    "    \n",
    "    return default() if isfunction(default) else default\n",
    "\n",
    "def Upsample(dim):\n",
    "    return nn.ConvTranspose2d(dim, dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "def Downsample(dim):\n",
    "    return nn.Conv2d(dim, dim, kernel_size=4, stride=2, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for time embedding a batch of time steps\n",
    "    into a tensor of shape (batch_size, dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time: torch.tensor, max_period: int = 10_000):\n",
    "        # time: (batch_size,)\n",
    "        # embeddings: (batch_size, dim)\n",
    "\n",
    "        # assert time.dim() == 1\n",
    "\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = time[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        \n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(0, 200)\n",
    "print(t.shape)\n",
    "sinusoidal_embedding = SinusoidalPositionEmbeddings(128)\n",
    "emb = sinusoidal_embedding(t)\n",
    "print(emb.shape)\n",
    "\n",
    "# plot the embeddings\n",
    "plt.imshow(emb.detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocks for U-net\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim_out, kernel_size=3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift: tuple = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
    "            if time_emb_dim is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb = None):\n",
    "        h = self.block1(x)\n",
    "\n",
    "        if self.mlp is not None and time_emb is not None:\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
    "\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
    "            if time_emb_dim is not None else None\n",
    "        )\n",
    "\n",
    "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
    "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, dim_out * mult),\n",
    "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        # print(x.shape)\n",
    "        h = self.ds_conv(x)\n",
    "        # print(h.shape)\n",
    "\n",
    "        if self.mlp is not None and time_emb is not None:\n",
    "            # print(\"Time emb shape in ConvNext:\", time_emb.shape)\n",
    "            condition = self.mlp(time_emb)\n",
    "            # print(condition.shape)\n",
    "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
    "\n",
    "        h = self.net(h)\n",
    "        return h + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.head = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1) # Learn Q, K, V jointly\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.head), qkv\n",
    "            )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "            )\n",
    "    \n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer for computing groupnorm before a function fn\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define U-net\n",
    "# takes in a batch of images: (batch_size, channels, height, width)\n",
    "# and returns a batch of images: (batch_size, channels, height, width)\n",
    "\n",
    "# Conv on input + positional embeddings\n",
    "# 2 x ConvNextBlock + GroupNorm + attention + residual + downsample x N\n",
    "# middle: ConvNeXT + attention \n",
    "# decoding: 2 x ConvNextBlock + GroupNorm + attention + residual + upsample x N\n",
    "# final: ConvNeXT + conv\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1,2,4,8),\n",
    "        channels=1,\n",
    "        resnet_block_groups=8,\n",
    "        convnext_mult=2,\n",
    "        use_convnext=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        init_dim = init_dim if init_dim else dim // 3 * 2\n",
    "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
    "\n",
    "        # create list of dimensions for each block\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        if use_convnext:\n",
    "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
    "        else:\n",
    "            block_klass = partial(ResBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # print(\"Using block class:\", block_klass)\n",
    "        # Time embedding\n",
    "        time_dim = dim * 4\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # create list of blocks\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out) # 'depth'\n",
    "\n",
    "        # Downsample\n",
    "        for i, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = i >= num_resolutions - 1\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList([\n",
    "                    block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                    block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                    Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                    Downsample(dim_out) if not is_last else nn.Identity(),\n",
    "                ]))\n",
    "        \n",
    "        # Middle part\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        # Upsample\n",
    "        for i, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = i >= num_resolutions - 1\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList([\n",
    "                    block_klass(dim_out*2, dim_in, time_emb_dim=time_dim),\n",
    "                    block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                    Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                    Upsample(dim_in) if not is_last else nn.Identity(),\n",
    "                ]))\n",
    "        \n",
    "        # Output\n",
    "        out_dim = out_dim if out_dim else channels\n",
    "        self.final_conv = nn.Sequential(\n",
    "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1))\n",
    "\n",
    "    def forward(self, x, time):\n",
    "\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        # print(\"Time shape:\", time.shape)\n",
    "        x = self.init_conv(x)\n",
    "        t = self.time_mlp(time)\n",
    "        h = [] # skip connections?\n",
    "\n",
    "        # Downsample\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        # Middle\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        # Upsample\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            x = upsample(x)\n",
    "        \n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call unet on image and time tensor\n",
    "\n",
    "\n",
    "t = torch.randint(0, 100, (batch_size,))\n",
    "\n",
    "x1 = next(iter(data_loader))[0]\n",
    "print(x1.shape)\n",
    "print(t.shape)\n",
    "\n",
    "unet = Unet(\n",
    "    dim=28,\n",
    "    channels=channels,\n",
    "    dim_mults=(1,2,4,),\n",
    "    resnet_block_groups=7,\n",
    "    use_convnext=False,\n",
    "    )\n",
    "\n",
    "out = unet(x1, t)\n",
    "\n",
    "# plot x1 and out side by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(x1[0, 0, :, :].detach().numpy())\n",
    "ax[1].imshow(out[0, 0, :, :].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "xsize = x1[0].size()\n",
    "tsize = t[0].size()\n",
    "# call summary on unet\n",
    "summary(unet, [(128, 1, 28, 28), (128,)], depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# use seed for reproducability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
    "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
    "\n",
    "    # Takes in list of (list of) tensor images (1, H, W) and plots them\n",
    "\n",
    "    # print(type(imgs[0]))\n",
    "    if isinstance(imgs[0], torch.Tensor):\n",
    "      imgs = [img.view(img.shape[1], img.shape[2]).numpy() for img in imgs]\n",
    "      \n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [image] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image = next(iter(data_loader))[0][0]\n",
    "print(image.shape)\n",
    "T = 200\n",
    "n_steps = 20\n",
    "noised_ims = [q_sample(image, torch.tensor([t])) for t in np.linspace(0, T-1, n_steps, dtype=int)\n",
    "]\n",
    "print(noised_ims[0].shape)\n",
    "plot(noised_ims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_losses(model, x_start, t, noise=None, loss_type=\"l1\"):\n",
    "    \"\"\" Computes the loss for a given model and batch of imgs.\n",
    "    Args:   x_start: the input image (batch_size, channels, height, width)\n",
    "            t: the time (batch_size,)\n",
    "            noise: the noise to add to the input image (batch_size, channels, height, width)\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    predicted_noise = model(x_noisy, t)\n",
    "\n",
    "    if loss_type == \"l1\":\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"l2\":\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise ValueError(\"Loss type not supported.\")\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = next(iter(data_loader))[0]\n",
    "print(im.shape)\n",
    "\n",
    "out = p_losses(unet, im, t, loss_type=\"huber\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling during training\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "    # Predict mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    # If we're on last sample\n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        return model_mean + noise * torch.sqrt(posterior_variance_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstep = torch.tensor([100])\n",
    "x100 = q_sample(im, tstep)\n",
    "print(x100.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = p_sample(unet, x100, t, 100)\n",
    "print(pred.shape)\n",
    "im_list = [x100[0, :, :, :], pred[0, :, :, :]]\n",
    "im_list[0].shape\n",
    "plot(im_list)\n",
    "\n",
    "# No noticable difference because small number of steps and model is not trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full sampling loop from noise to image\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for i in tqdm(reversed(range(0, T)), desc=\"Sampling\"):\n",
    "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=1):\n",
    "    return p_sample_loop(model, (batch_size, channels, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "results_folder = Path(\"./results\")\n",
    "results_folder.mkdir(exist_ok=True)\n",
    "save_and_sample_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "unet.to(device)\n",
    "\n",
    "optimizer = Adam(unet.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "from tqdm import trange\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "with trange(epochs) as pbar:\n",
    "    for epoch in pbar:\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Actually loading images and labels now in data_loader...\n",
    "\n",
    "            batch_size = batch[0].shape[0]\n",
    "            batch = batch[0].to(device)\n",
    "\n",
    "            # Sample t uniformally \n",
    "            t = torch.randint(0, T, (batch_size,), device=device).long()\n",
    "\n",
    "            loss = p_losses(unet, batch, t, loss_type=\"huber\")\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                pbar.set_postfix(step=step, loss=loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save generated images\n",
    "            if step != 0 and step % save_and_sample_every == 0:\n",
    "                milestone = step // save_and_sample_every\n",
    "                batches = num_to_groups(4, batch_size)\n",
    "                all_images_list = list(map(lambda n: sample(unet, batch_size=n), batches))\n",
    "                all_images = torch.cat(all_images_list, dim=0)\n",
    "                save_image(all_images, str(results_folder / f\"sample_{milestone}.png\"), nrow=6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4379223da8825d70b8c2eb245c1476bc432c86a2a66eb506943eca18af8434f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
